{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build a test plan with GPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain openai langchain_openai\n",
    "%pip install python-dotenv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#test plan history\n",
    "plan_columns = [\"Data\", \"Version\", \"Name\", \"Description\", \"Status\", \"Owner\"]\n",
    "plan_histroy = [\n",
    "    \"Data=07/10/2024; Version=0.5; Name=aaa; Description=bbb; Status=ccc; Owner=ddd\",\n",
    "    \"Data=07/11/2024; Version=0.6; Name=aaa; Description=bbb; Status=ccc; Owner=ddd\",\n",
    "    \"Data=07/12/2024; Version=0.7; Name=aaa; Description=bbb; Status=ccc; Owner=ddd\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#project description\n",
    "project_description = \"\"\"The PCIe Lane Margining Service is designed to enhance the reliability and performance of PCIe connections within Intel's computing environments. This service aims to systematically assess and optimize the margining of PCIe lanes, ensuring robust data transmission and minimizing the risk of data integrity issues caused by signal degradation.\n",
    "\n",
    "Objectives\n",
    "1.Performance Optimization: Improve the overall performance of PCIe lanes by identifying and adjusting the margins to optimal levels.\n",
    "2.Reliability Enhancement: Increase the reliability of PCIe connections by proactively detecting and correcting potential signal integrity issues.\n",
    "3.Automated Testing: Implement automated testing procedures to continuously monitor the health and performance of PCIe lanes across various systems.\n",
    "4.Data Analysis: Utilize advanced data analysis techniques to interpret test results and make informed decisions about margin adjustments.\n",
    "\n",
    "Key Features\n",
    "1.Automated Margin Testing: Automated scripts and tools to perform margin testing on PCIe lanes, providing regular updates on lane performance and integrity.\n",
    "2.Real-Time Monitoring: Real-time monitoring capabilities to track the status of PCIe lanes and alert administrators about potential issues.\n",
    "3.Adjustment Algorithms: Advanced algorithms to automatically adjust margins based on real-time data and predefined performance thresholds.\n",
    "4.Reporting Tools: Comprehensive reporting tools that generate detailed reports on margining activities, outcomes, and recommendations for further improvements.\n",
    "5.Integration with Existing Systems: Seamless integration with existing Intel system architectures and management tools to ensure smooth operation and minimal disruption. \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to access the openAI API, you need an API key which you can get it from  https://platform.openai.com/api-keys\n",
    "once you have a key then you can either save it as an environment variable or hard code in as needed from simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import openai\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load the OpenAI API key from the environment variables\n",
    "try:\n",
    "    load_dotenv(override=True)\n",
    "    # os.environ[\"OPENAI_API_KEY\"] = \"your key\"\n",
    "    openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "    if not openai.api_key:\n",
    "        raise ValueError(\"OPENAI_API_KEY is not set in the environment variables.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading environment variables or setting OpenAI API key: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "initialize OpenAI model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openAI model name\n",
    "# https://platform.openai.com/docs/models\n",
    "#openAI_model=\"gpt-4\"\n",
    "#openAI_model=\"gpt-4-32k\"\n",
    "#openAI_model=\"gpt-4-turbo\"\n",
    "openAI_model=\"gpt-4o\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "try:\n",
    "    prompt = PromptTemplate(\n",
    "        input_variables=[\"chat_history\", \"question\"],\n",
    "        template=\"\"\"You are software test specialist who has a high level of experience in software quality assurance and software testing for the C language or C++ language or Python language.  \n",
    "        Software testing encompasses various crucial aspects such as usability, functionality, performance, \n",
    "        security, and many more. Thus, rigorous testing protocols enable testers to assess and remove vulnerabilities \n",
    "        and shortcomings which may affect the end users. As a software test specialist, you are responsible writing test plans and test cases and test code.\n",
    "        For each test plan, you can write multiple test cases.\n",
    "\n",
    "        chat_history: {chat_history}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "    )\n",
    "\n",
    "    llm = ChatOpenAI(openai_api_key=os.environ[\"OPENAI_API_KEY\"], model_name=openAI_model)\n",
    "\n",
    "    memory = ConversationBufferWindowMemory(memory_key=\"chat_history\", k=4)\n",
    "    llm_chain = LLMChain(\n",
    "        llm=llm,\n",
    "        memory=memory,\n",
    "        prompt=prompt\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Failed to initialize ChatOpenAI : {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "                {\"role\": \"assistant\", \"content\": \"Hello there\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "user_prompt = \"what can you do?\"\n",
    "messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "ai_response = llm_chain.predict(question=user_prompt)\n",
    "messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description=\"indicate the purpose of the plan, the level of the plan, determine the scope of the project, the amount of testing effort, indicate how testing is related to other evaluation activities, indicate the possible processes that will be used for control of changes and communication, coordination of key activities\"\n",
    "user_prompt = \"write introduction section according to the task and project description. \" \\\n",
    "    + \"\\n\" + \"task: \" + task_description \\\n",
    "    + \"\\n\" + \"project description: \" + project_description\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "ai_response = llm_chain.predict(question=user_prompt)\n",
    "messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description=\"\"\"write a listing of what is to be tested from the USERS viewpoint of what the system does. This is not a technical description of the software, but a USERS view of the functions. Set the level of risk for each feature. Use a simple rating scale such as (H, M, L): High, Medium and Low. These types of levels are understandable to a User. You should be prepared to discuss why a particular level was chosen. Users do not understand technical software terminology; they understand functions and processes as they relate to their jobs.\"\"\"\n",
    "user_prompt = \"write test scope section according to the task and project description. \" \\\n",
    "    + \"\\n\" + \"task: \" + task_description \\\n",
    "    + \"\\n\" + \"project description: \" + project_description\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "ai_response = llm_chain.predict(question=user_prompt)\n",
    "messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description=\"\"\"write the overall test strategy for this test plan; it should be appropriate to the level of the plan (master, acceptance, etc.) and should be in agreement with all higher and lower levels of plans. Overall rules and processes should be identified. Are any special tools to be used and what are they? Will the tool require special training? What metrics will be collected? Which level is each metric to be collected at? How is Configuration Management to be handled? How many different configurations will be tested? Hardware. Software. Combinations of HW, SW and other vendor packages. What levels of regression testing will be done and how much at each test level? Will regression testing be based on severity of defects detected? How will elements in the requirements and design that do not make sense or are untestable be processed? If this is a master test plan the overall project testing approach and coverage requirements must also be identified. Specify if there are special requirements for the testing. Only the full component will be tested. A specified segment of grouping of features/components must be tested together. MTBF, Mean Time Between Failures - if this is a valid measurement for the test involved\n",
    "and if the data is available. SRE, Software Reliability Engineering - if this methodology is in use and if the information is available. How will meetings and other organizational processes be handled?.\"\"\"\n",
    "user_prompt = \"write test strategy section according to the task and project description. \" \\\n",
    "    + \"\\n\" + \"task: \" + task_description \\\n",
    "    + \"\\n\" + \"project description: \" + project_description\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "ai_response = llm_chain.predict(question=user_prompt)\n",
    "messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pass fail criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description=\"\"\"write completion criteria for this plan. This is a critical aspect of any test plan and should be appropriate to the level of the plan. At the Unit test level this could be items such as: All test cases completed. A specified percentage of cases completed with a percentage containing some number of minor defects. Code coverage tool indicates all code covered. At the Master test plan level this could be items such as: All lower level plans completed. A specified number of plans completed without errors and a percentage with minor defects. This could be an individual test case level criterion or a unit level plan or it can be general functional requirements for higher level plans. What is the number and severity of defects located? Is it possible to compare this to the total number of defects? This may be impossible, as some defects are never detected. A defect is something that may cause a failure, and may be acceptable to leave in the application. A failure is the result of a defect as seen by the User, the system crashes, etc.\"\"\"\n",
    "user_prompt = \"write pass/fail criteria section according to the task and project description. \" \\\n",
    "    + \"\\n\" + \"task: \" + task_description \\\n",
    "    + \"\\n\" + \"project description: \" + project_description\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "ai_response = llm_chain.predict(question=user_prompt)\n",
    "messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deliverables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description=\"\"\" describe what is to be delivered as part of this plan. Test plan document. Test cases. Test design specifications. Tools and their outputs. Simulators. Static and dynamic generators. Error logs and execution logs. Problem reports and corrective actions. One thing that is not a test deliverable is the software itself that is listed under test items and is delivered by development.\"\"\"\n",
    "user_prompt = \"write 'test deliverables' section according to the task and project description. \" \\\n",
    "    + \"\\n\" + \"task: \" + task_description \\\n",
    "    + \"\\n\" + \"project description: \" + project_description\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "ai_response = llm_chain.predict(question=user_prompt)\n",
    "messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unit testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description=\"\"\"write the executible testing plan of individual units or components of a software application to ensure they function correctly in isolation.\n",
    "                you should write test plan based on the project description that meets the following constraints and requirements.            \n",
    "                Test plan Introduction section provides an overview of the entire test plan. \n",
    "                Test case id section provides a unique identifier for the test case.\n",
    "                Test case description section provides a description of the test case.\n",
    "                Test case steps section outlines the steps that will be taken to conduct the test case.\n",
    "                you should implement test code  to call the function with the provided input for each test step. The code follows industry's style guide and is easy to understand and maintain.\n",
    "                Test case expected results section outlines the expected results of the test case.\n",
    "                Test case actual results section outlines the actual results of the test case.\n",
    "                Test case status section outlines the status of the test case.\n",
    "                Test execution section outlines the execution of the testing process.\n",
    "                Data analysis and reporting section outlines the analysis and reporting of the testing process.\n",
    "                Test deliverables section outlines the deliverables of the testing process.\n",
    "\"\"\"\n",
    "user_prompt = \"write 'unit testing' section according to the task and project description. \" \\\n",
    "    + \"\\n\" + \"task: \" + task_description \\\n",
    "    + \"\\n\" + \"project description: \" + project_description\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "ai_response = llm_chain.predict(question=user_prompt)\n",
    "messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "integration testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description=\"\"\"write the testing of combined units or systems to verify their interactions and interfaces function as expected.\n",
    "                you should write test plan based on the project description that meets the following constraints and requirements.            \n",
    "                Test plan Introduction section provides an overview of the entire test plan. \n",
    "                Test case id section provides a unique identifier for the test case.\n",
    "                Test case description section provides a description of the test case.\n",
    "                Test case steps section outlines the steps that will be taken to conduct the test case.\n",
    "                you should implement test code  to call the function with the provided input for each test step. The code follows industry's style guide and is easy to understand and maintain.\n",
    "                Test case expected results section outlines the expected results of the test case.\n",
    "                Test case actual results section outlines the actual results of the test case.\n",
    "                Test case status section outlines the status of the test case.\n",
    "                Test execution section outlines the execution of the testing process.\n",
    "                Data analysis and reporting section outlines the analysis and reporting of the testing process.\n",
    "                Test deliverables section outlines the deliverables of the testing process.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"write 'Integration testing' section according to the task and project description. \" \\\n",
    "    + \"\\n\" + \"task: \" + task_description \\\n",
    "    + \"\\n\" + \"project description: \" + project_description\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "ai_response = llm_chain.predict(question=user_prompt)\n",
    "messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "print(ai_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "system testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_description=\"\"\"write The testing of a complete and integrated software system to evaluate its compliance with specified requirements\n",
    "                you should write test plan based on the project description that meets the following constraints and requirements.            \n",
    "                Test plan Introduction section provides an overview of the entire test plan. \n",
    "                Test case id section provides a unique identifier for the test case.\n",
    "                Test case description section provides a description of the test case.\n",
    "                Test case steps section outlines the steps that will be taken to conduct the test case.\n",
    "                you should implement test code  to call the function with the provided input for each test step. The code follows industry's style guide and is easy to understand and maintain.\n",
    "                Test case expected results section outlines the expected results of the test case.\n",
    "                Test case actual results section outlines the actual results of the test case.\n",
    "                Test case status section outlines the status of the test case.\n",
    "                Test execution section outlines the execution of the testing process.\n",
    "                Data analysis and reporting section outlines the analysis and reporting of the testing process.\n",
    "                Test deliverables section outlines the deliverables of the testing process.\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = \"write 'system testing' section according to the task and project description. \" \\\n",
    "    + \"\\n\" + \"task: \" + task_description \\\n",
    "    + \"\\n\" + \"project description: \" + project_description\n",
    "\n",
    "messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "ai_response = llm_chain.predict(question=user_prompt)\n",
    "messages.append({\"role\": \"assistant\", \"content\": ai_response})\n",
    "\n",
    "print(ai_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
